{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name: Seyyid Osman Sevgili\n",
    "# ID: 504221565"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hw I - Fully Connected Neural Networks (100 Points)\n",
    "\n",
    "We covered artificial neural networks with multiple hidden layers in class. In this assignment, you will implement Fully Connected Neural Network (FCN) components in order to perform a supervised classification task.\n",
    "\n",
    "The dataset you are going to work with are : (i) for development of your code, you will use diamonds dataset for classification; (ii) for actual training and testing of your implementation in this assignment.\n",
    "\n",
    "Usage of any built-in functions for code parts that you are asked to write are not allowed. We provide a skeleton code on which to build on your own architecture. In the Layer class, there are two important methods, named as forward and backward. Almost everything you will use in this assignment is derived from this class. We will follow PyTorch-like architecture in the skeleton code.\n",
    "\n",
    "**Please do not modify the following cells. We will use them for the evaluation of your homeworks. **\n",
    "\n",
    "**You should modify and fill in the code under DL/layers.py, which includes functions such as layer.NNLayer.* ...**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from DL import bases, layers, activations, losses, classifiers, optimizers\n",
    "from DL.checker.checks import *\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You should read:** https://karpathy.medium.com/yes-you-should-understand-backprop-e2f06eab496b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To auto-reload your modules from the *.py files, re run the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Layers, Activations and Optimizers\n",
    "\n",
    "In the `Layer` class, there are two important methods, named as `forward` and `backward`. Almost everything you will use in this assignment is derived from this class. You will be programming in Python language.\n",
    "\n",
    "**Don't forget to test your implementation by using the cells below!**\n",
    "\n",
    "\n",
    "\n",
    "### a. Affine Layer \n",
    "\n",
    "In this layer, we basically implement the hidden layers of neural nets. Each neuron (building block of neural networks) is a just logistic regression classifier itself, but stacking these neurons make them powerful to implement any function.\n",
    "We are going to implement our affine layer \n",
    "\n",
    "Go under `DL/layers.py` and find `AffineLAyer` class. Implement the forward pass for Affine layer which is formulated as follows:\n",
    "\n",
    "$ z = W x + b $ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs = 10\n",
    "input_shape = (4, 7, 2) \n",
    "output_dim = 3\n",
    "\n",
    "input_size = num_inputs * np.prod(input_shape)\n",
    "weight_size = output_dim * np.prod(input_shape)\n",
    "affineLayer = layers.AffineLayer(input_size, weight_size)\n",
    "\n",
    "x = np.linspace(-0.1, 0.5, num=input_size).reshape(num_inputs, *input_shape)\n",
    "affineLayer.W = np.linspace(-0.2, 0.3, num=weight_size).reshape(np.prod(input_shape), output_dim)\n",
    "affineLayer.b = np.linspace(-0.3, 0.1, num=output_dim)\n",
    "\n",
    "out = affineLayer.forward(x)\n",
    "relError = rel_error(out, \"affine_forward\")\n",
    "\n",
    "print('Testing forward method of affine layer:')\n",
    "print(f'difference: {relError}')\n",
    "assert 1e-6 > relError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backward pass : \n",
    "Go under `DL/layers.py` and find `AffineLayer` class. Implement the backward pass for Affine layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(250)\n",
    "num_inputs = 7\n",
    "input_shape = (4, 10, 3)\n",
    "output_dim = 8\n",
    "\n",
    "input_size = num_inputs * np.prod(input_shape)\n",
    "weight_size = output_dim * np.prod(input_shape)\n",
    "affineLayer = layers.AffineLayer(input_size, weight_size)\n",
    "\n",
    "x = np.random.randn(13, 2, 3)\n",
    "affineLayer.W = np.random.randn(6, 5)\n",
    "affineLayer.b = np.random.randn(5)\n",
    "dout = np.random.randn(13, 5)\n",
    "\n",
    "dx_num = grad_check(affineLayer.forward, x, dout)\n",
    "dw_num = grad_check(lambda _ : affineLayer.forward(x), affineLayer.W, dout)\n",
    "db_num = grad_check(lambda _ : affineLayer.forward(x), affineLayer.b, dout)\n",
    "\n",
    "affineLayer.forward(x)\n",
    "dx, dw, db = affineLayer.backward(dout)\n",
    "\n",
    "# Errors should be around 1e-6 at least\n",
    "print('Testing backward method of affine layer:')\n",
    "print(f'dx error: {rel_error(dx_num, dx)}')\n",
    "print(f'dw error: {rel_error(dw_num, dw)}')\n",
    "print(f'db error: {rel_error(db_num, db)}')\n",
    "\n",
    "assert 1e-6 > rel_error(dx_num, dx) \n",
    "assert 1e-6 > rel_error(dw_num, dw) \n",
    "assert 1e-6 > rel_error(db_num, db) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Activation Layers "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go under `DL/activations.py` and find `ExponentialLinearUnit` class. Implement the forward pass for ExponentialLinearUnit:\n",
    "\n",
    "ELU(x) = \n",
    "\\begin{cases} \n",
    "x & \\text{if } x > 0 \\\\\n",
    "\\alpha (e^x - 1) & \\text{if } x \\leq 0\n",
    "\\end{cases}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exponentialLinearUnit = activations.ExponentialLinearUnit(1.0)\n",
    "x = 2*(np.array([0,1,3,4,-1,2,4,1773,-1773, 1.3, .4, -.1]).reshape(3, -1))\n",
    "out = exponentialLinearUnit.forward(x)\n",
    "import numpy as np\n",
    "\n",
    "# Change print format settings\n",
    "np.set_printoptions(suppress=True, precision=3)\n",
    "print(np.array(out))\n",
    "\n",
    "# Compare your output with ours. \n",
    "relError = rel_error(out, \"ExponentialLinearUnit_forward\")\n",
    "print('Testing forward method of ExponentialLinearUnit layer:')\n",
    "print(f'Error: {relError}')\n",
    "assert 1e-6 > relError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exponentialLinearUnit = activations.ExponentialLinearUnit(1.0)\n",
    "np.random.seed(1773)\n",
    "x = np.random.randn(10, 10)\n",
    "dout = np.random.randn(*x.shape)\n",
    "\n",
    "dx_num = grad_check(exponentialLinearUnit.forward, x, dout)\n",
    "\n",
    "exponentialLinearUnit.forward(x)\n",
    "dx = exponentialLinearUnit.backward(dout)\n",
    "\n",
    "\n",
    "# The error should be around 2e-11\n",
    "print('Testing backward method of exponentialLinearUnit layer:')\n",
    "print(f'dx error: {rel_error(dx_num, dx)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Softmax classifier  \n",
    "\n",
    "In multi-class classification task, as we've seen in the class, the softmax loss function is utilized. \n",
    "Practically, at the final layer of the network, instead of the standard activation, we utilize softmax function to turn the likelihood of each class into class probabilities. Then, we utilize the cross-entropy loss as the data loss. Below, you implement and return only the data loss component in your overall loss. \n",
    "\n",
    "***Implement your loss computation in the function `loss` of the `DL/losses.py`***\n",
    "\n",
    "You will write forward pass and backward pass for the softmax unit. Below, we evaluate your method by a numerical gradient method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1773)\n",
    "num_classes, num_inputs = 8, 60\n",
    "x = 0.001 * np.random.randn(num_inputs, num_classes)\n",
    "y = np.random.randint(num_classes, size=num_inputs)\n",
    "softmax = classifiers.Softmax()\n",
    "\n",
    "def softmax_loss(x,y):\n",
    "    probs = softmax.forward(x)\n",
    "    dx = softmax.backward(y)\n",
    "    loss = losses.loss(probs, y) \n",
    "    return loss,dx\n",
    "\n",
    "\n",
    "loss, dx = softmax_loss(x,y)\n",
    "print(loss, dx.shape)\n",
    "dx_num = grad_check(lambda x: softmax_loss(x, y)[0], x)\n",
    "\n",
    "# The loss should be about 2.0\n",
    "relError = rel_error(dx_num, dx)\n",
    "print('Testing softmax_loss:')\n",
    "print(f'loss: {loss}')\n",
    "print(f'dx error: {relError}')\n",
    "assert 3 > loss\n",
    "assert 1e-6 > relError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d. Implement RmsProp optimizer \n",
    "Implement RmsProp Strategy in 'Rmsprop' classes. Test their correctness using the cell below. \n",
    "Implement `RmsProp` class in `DL/optimizers.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1773)\n",
    "toyModel = bases.Model()\n",
    "model_layers = [layers.AffineLayer(10,2, seed=1773), layers.AffineLayer(2,3, seed=1773), classifiers.Softmax()]\n",
    "toyModel(model_layers)\n",
    "optimizer = optimizers.RMSprop(model=toyModel, lr=1, beta=0.9, epsilon=1e-8)\n",
    "\n",
    "x = np.random.randn(3,10)\n",
    "y = np.array([0,1,2]).reshape(-1)\n",
    "toyModel.forward(x)\n",
    "toyModel.backward(y)\n",
    "optimizer.optimize()\n",
    "\n",
    "student_out = []\n",
    "for i in range(2):\n",
    "    student_out.append(toyModel[i].W)\n",
    "    student_out.append(toyModel[i].b)\n",
    "    \n",
    "for i in range(4):\n",
    "    layer_name = [\"weights\", \"biases\"][i%2]\n",
    "    relError = rel_error(student_out[i], f\"RMSprop_{layer_name}_{i//2}\")\n",
    "    print(f'Testing {layer_name} of {i//2}th layer')\n",
    "    assert 1e-6 > relError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build and test your own model! (85 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an example which is implemented using previously defined API. In this example, you will use a subset of diamonds dataset (https://github.com/mwaskom/seaborn-data/blob/master/diamonds.csv). Each instance has 4 features in this subset. You will select a pair of two features and do your experiments on it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "diamonds = sns.load_dataset(\"diamonds\")\n",
    "\n",
    "# create a subset\n",
    "diamonds_filtered = (diamonds[diamonds['color'].isin(['E', 'J', 'H'])]).iloc[:500]\n",
    "diamonds_filtered = diamonds_filtered[['carat','depth', 'table', 'color', 'price']]\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(diamonds_filtered.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(diamonds_filtered,hue='color')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment:\n",
    "Consider any pair of features? Do you think they are linearly separable? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#comment here. which 2D projections seems easier to work on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare your data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "# choose a pair of features to form data X. Convert the diamonds color label to index label y. \n",
    "# Drop NANs\n",
    "data = #TODO\n",
    "# See how much data points you have \n",
    "# Prepare your data. Choose two feature columns. and get target labels as an integer or one-hot encoded value\n",
    "X, y =  # # TODO Get the features and the corresponding classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### f. Model and Training (15pts) \n",
    "Create a one hidden layer multilayer neural network model, train and test. \n",
    "* Obtain at least 70% test accuracy in 30 epochs, at most.\n",
    "* Tune the variables as you wish. \n",
    "* For activations, optimizers you can choose any. But the following question asks you to compare cases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the code below\n",
    "\n",
    "model = bases.Model() # Create a model instance\n",
    "\n",
    "# The dataset must have 2 features, so the input size of first layer is 2. We have 3 classes, so size of last hidden is 3. \n",
    "# Each neuron corresponds the likelihood of a class, named P(y=neuron_index|x), where y is class label \n",
    "# and x is features given.\n",
    "\n",
    "# Setup the model\n",
    "# model_layers = a list of layers \n",
    "\n",
    "model_layers = # TODO a list of layers \n",
    "\n",
    "model(model_layers) # Load layers to model object\n",
    "predictions  = np.ones(y.shape[0]) # Number of instances in the diamonds data\n",
    "train_accs = []\n",
    "test_accs = []\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "# Shuffle dataset\n",
    "def create_permutation(x, y):\n",
    "    perm = np.random.permutation(len(x))\n",
    "    return x[perm], y[perm]\n",
    "\n",
    "def train_test_split(X, y, ratio=.2):\n",
    "    X, y = create_permutation(X, y)\n",
    "    split_index =  int(len(X) * (1-ratio))\n",
    "    X_train, y_train = X[:split_index], y[:split_index]\n",
    "    X_test, y_test = X[split_index:], y[split_index:]\n",
    "    return X_train, y_train, X_test, y_test\n",
    "    \n",
    "\n",
    "# Options\n",
    "preprocessing_on = False\n",
    "shuffle_on_each_epoch = True\n",
    "regularization_strength = # TODO\n",
    "n_epochs = # TODO\n",
    "train_test_split_ratio = .2\n",
    "print_every = # TODO\n",
    "test_every = # TODO\n",
    "test_points = []\n",
    "if preprocessing_on:\n",
    "    X = preprocessing.scale(X)\n",
    "X_train, y_train, X_test, y_test = train_test_split(X, y)\n",
    "print(X_train.shape, y_train.shape)\n",
    "\n",
    "optimizer = # TODO\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    if shuffle_on_each_epoch:\n",
    "        X_train, y_train = create_permutation(X_train, y_train)\n",
    "\n",
    "    # COMPLETE THE CODE BELOW \n",
    "    softmax_out = # TODO\n",
    "\n",
    "\n",
    "    predictions = # TODO\n",
    "    loss = # TODO\n",
    "    train_acc = # TODO\n",
    "    train_accs.append(train_acc)\n",
    "    train_losses.append(loss)\n",
    "    \n",
    "    if epoch % print_every == 0:\n",
    "        print(f\"TRAIN:\\tEpoch: {epoch:3d}, Loss: {loss:.5f}, Accuracy: {train_acc:.5f}\")\n",
    "    \n",
    "    # call backpropagation\n",
    "    #\n",
    "    #\n",
    "\n",
    "    # Then complete the testing part.\n",
    "    if epoch % test_every == 0 or epoch == n_epochs - 1:\n",
    "        softmax_out = # TODO\n",
    "        predictions = # TODO\n",
    "        loss = # TODO\n",
    "        test_acc = # TODO\n",
    "        test_losses.append(loss)\n",
    "        test_points.append(epoch)\n",
    "        test_accs.extend([test_acc for i in range(test_every)])\n",
    "        print(f\"TEST:\\tEpoch: {epoch:3d}, Loss: {loss:.5f}, Accuracy: {test_acc:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### f. Plot the training and test loss curves for diagnostics below (10 pts):\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# insert your code below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### g1. Compare the performances of at least three different activation functions.  (10 pts)\n",
    "Comment on the result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the code "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### g2. Compare the performances of different learning rates.  (10 pts)\n",
    "Comment on the result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the code "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### h. Plot the confusion Matrix (10 points)\n",
    "Plot the confusion matrix and comment on the result (for your best result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "# insert your code below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### i. Plot the decision boundary (10 points)\n",
    "Here, plot the decision boundary of your best result on the feature space. Revisit the feature pair scatter plot of your chosen feature pair. Highlight different classes using different classes and markers. \n",
    "Make sure you can identify training and testing datasets separately. Alternatively you can plot two different scatter plots. The classification boundaries are the points on feature (input) space where any two classes have equal (or near equal probability). Also highlight misclassified samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train neural network model and classifier\n",
    "# (Assuming you have already trained the model and classifier)\n",
    "# insert your code below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k. Comment how does using different activation functions affect decision boundary? (5pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show some plots and comment on them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### l. Original diamonds dataset can also be classified according to different categories: cut and clarity. Which label is easier to classify for your model?  (15 pts)\n",
    "\n",
    "Change the filtering to create different subsets with different labels, \"here\". Filter the subset to have three classes, 500 samples and 2 features per sample. Then, classify the samples. Comment on the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert your codes and write your comments here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Pytorch (15 pts)\n",
    "#### Repeat the model construction and training/testing (.f) with Pytorch layers and activations and optimizers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
